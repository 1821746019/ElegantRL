{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_helloworld_DQN_DDPG_PPO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/ElegantRL/blob/master/tutorial_helloworld_DQN_DDPG_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# Demo: ElegantRL_HelloWorld_tutorial (DQN --> DDPG --> PPO)\n",
        "\n",
        "We suggest to following this order to quickly learn about RL:\n",
        "- DQN (Deep Q Network), a basic RL algorithms in discrete action space.\n",
        "- DDPG (Deep Deterministic Policy Gradient), a basic RL algorithm in continuous action space.\n",
        "- PPO (Proximal Policy Gradient), a widely used RL algorithms in continuous action space.\n",
        "\n",
        "If you have any suggestion about ElegantRL Helloworld, you can discuss them in [ElegantRL issues/135: Suggestions for elegant_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135), and we will keep an eye on this issue.\n",
        "ElegantRL's code, especially the Helloworld, really needs a lot of feedback to be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 1: Install ElegantRL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U35bhkUqOqbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c8ea84-e520-4058-cb62-f80192aa371e"
      },
      "source": [
        "# install elegantrl library\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-s3u140ew\n",
            "  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-s3u140ew\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.21.6)\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 109 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (1.10.0+cu111)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from elegantrl==0.3.3) (4.1.2.30)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->elegantrl==0.3.3) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->elegantrl==0.3.3) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->elegantrl==0.3.3) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->elegantrl==0.3.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->elegantrl==0.3.3) (1.15.0)\n",
            "Building wheels for collected packages: elegantrl\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.3-py3-none-any.whl size=239162 sha256=b45d23493b8f7bbe8db6c324e918a05b5930865c6b03dafda838ef3041dc2059\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rw5d1bev/wheels/52/9a/b3/08c8a0b5be22a65da0132538c05e7e961b1253c90d6845e0c6\n",
            "Successfully built elegantrl\n",
            "Installing collected packages: pybullet, box2d-py, elegantrl\n",
            "Successfully installed box2d-py-2.3.8 elegantrl-0.3.3 pybullet-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Part 2: Import ElegantRL helloworld**\n",
        "\n",
        "We hope that the `ElegantRL Helloworld` would help people who want to learn about reinforcement learning to quickly run a few introductory examples.\n",
        "- **Less lines of code**. (code lines <1000)\n",
        "- **Less packages requirements**. (only `torch` and `gym` )\n",
        "- **keep a consistent style with the full version of ElegantRL**.\n",
        "\n",
        "![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/figs/File_structure.png)\n",
        "\n",
        "One sentence summary: an agent `agent.py` with Actor-Critic networks `net.py` is trained `run.py` by interacting with an environment `env.py`.\n",
        "\n",
        "\n",
        "In this tutorial, we only need to download the directory from [elegantrl_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld) using the following code.\n",
        "\n",
        "The files in `elegantrl_helloworld` including:\n",
        "`config.py`, `agent.py`, `net.py`, `env.py`, `run.py`"
      ],
      "metadata": {
        "id": "zJPivVxHMrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r -f /content/elegantrl_helloworld  # remove if the directory exists\n",
        "!wget https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/elegantrl_helloworld -P /content/"
      ],
      "metadata": {
        "id": "sw_gE-IpovQ4",
        "outputId": "2e3da312-cd22-4342-fa5c-ed77259a2939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-20 08:04:30--  https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/elegantrl_helloworld\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld [following]\n",
            "--2022-04-20 08:04:30--  https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/elegantrl_helloworld’\n",
            "\n",
            "elegantrl_helloworl     [ <=>                ] 114.22K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-04-20 08:04:30 (2.54 MB/s) - ‘/content/elegantrl_helloworld’ saved [116960]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.config import Arguments"
      ],
      "metadata": {
        "id": "nweGpiR1M0yA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdmpnK_3Zcn"
      },
      "source": [
        "## **Part 3: Train DQN on discreted action space task.**\n",
        "\n",
        "Train DQN on [**Discreted action** space task `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "You can see [/elegantrl_helloworld/config.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl_helloworld/config.py) to get more information about hyperparameter.\n",
        "\n",
        "```\n",
        "class Arguments:\n",
        "    def __init__(self, agent_class, env_func=None, env_args=None):\n",
        "        self.env_num = self.env_args['env_num']  # env_num = 1. In vector env, env_num > 1.\n",
        "        self.max_step = self.env_args['max_step']  # the max step of an episode\n",
        "        self.env_name = self.env_args['env_name']  # the env name. Be used to set 'cwd'.\n",
        "        self.state_dim = self.env_args['state_dim']  # vector dimension (feature number) of state\n",
        "        self.action_dim = self.env_args['action_dim']  # vector dimension (feature number) of action\n",
        "        self.if_discrete = self.env_args['if_discrete']  # discrete or continuous action space\n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"CartPole-v0\"\n",
        "\n",
        "import gym\n",
        "gym.logger.set_level(40)  # Block warning\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
        "args.gamma = 0.97  # discount factor of future rewards\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 2  # collect target_step, then update network\n",
        "args.net_dim = 2 ** 7  # the middle layer dimension of Fully Connected Network\n",
        "args.num_layer = 3  # the layer number of MultiLayer Perceptron, `assert num_layer >= 2`\n",
        "args.batch_size = 2 ** 7  # num of transitions sampled from replay buffer.\n",
        "args.repeat_times = 2 ** 0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
        "args.explore_rate = 0.25  # epsilon-greedy for exploration.\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 5  # number of times that get episode return\n",
        "args.eval_times = 2 ** 3  # number of times that get episode return\n",
        "args.break_step = int(8e4)  # break training if 'total_step > break_step'"
      ],
      "metadata": {
        "id": "AAPdjovQrTpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df46a9ae-4d8c-4836-b471-f755282a5393"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env_args = {'env_num': 1,\n",
            "            'env_name': 'CartPole-v0',\n",
            "            'max_step': 200,\n",
            "            'state_dim': 4,\n",
            "            'action_dim': 2,\n",
            "            'if_discrete': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose gpu id `0` using `args.learner_gpu = 0`. Set as `-1` or GPU is unavaliable, the training program will choose CPU automatically.\n",
        "\n",
        "- The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200) \n",
        "- The cumulative returns of task_name is ∈ (min score, (score of random action, target score), max score)."
      ],
      "metadata": {
        "id": "Rq5LPOH2B0aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args.learner_gpus = -1\n",
        "\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7SBwVAkA8lA",
        "outputId": "a0385b35-5886-4a96-c55c-3d19606f9bb8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Arguments Keep cwd: ./CartPole-v0_DQN_-1\n",
            "\n",
            "| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\n",
            "| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\n",
            "| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\n",
            "| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\n",
            "\n",
            "| Steps 4.08e+02  ExpR     1.00  | ObjC     0.08  ObjA     0.89\n",
            "| Steps 1.79e+04  ExpR     1.00  | ObjC     0.25  ObjA    16.98\n",
            "| Steps 3.13e+04  ExpR     1.00  | ObjC     0.32  ObjA    26.03\n",
            "| Steps 4.11e+04  ExpR     1.00  | ObjC     0.93  ObjA    29.85\n",
            "| Steps 4.98e+04  ExpR     1.00  | ObjC     1.19  ObjA    31.70\n",
            "| Steps 5.72e+04  ExpR     1.00  | ObjC     0.06  ObjA    30.98\n",
            "| Steps 6.43e+04  ExpR     1.00  | ObjC     0.21  ObjA    31.57\n",
            "| Steps 7.06e+04  ExpR     1.00  | ObjC     0.81  ObjA    31.66\n",
            "| Steps 7.59e+04  ExpR     1.00  | ObjC     0.58  ObjA    32.71\n",
            "| Steps 8.03e+04  ExpR     1.00  | ObjC     0.49  ObjA    30.81\n",
            "| UsedTime: 299 | SavedDir: ./CartPole-v0_DQN_-1\n",
            "| Arguments Keep cwd: ./CartPole-v0_DQN_-1\n",
            "\n",
            "| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\n",
            "| `avgR` denotes average value of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `stdR` denotes standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `avgS` denotes the average number of steps in an episode.\n",
            "\n",
            "| Steps 4.02e+02  | avgR     9.250  stdR     0.829  | avgS      9\n",
            "| Steps 4.08e+02  | avgR     9.125  stdR     0.599  | avgS      9\n",
            "| Steps 1.79e+04  | avgR    95.375  stdR     8.230  | avgS     95\n",
            "| Steps 1.85e+04  | avgR    99.625  stdR     7.777  | avgS    100\n",
            "| Steps 3.05e+04  | avgR    84.250  stdR     5.673  | avgS     84\n",
            "| Steps 3.13e+04  | avgR   133.625  stdR     4.442  | avgS    134\n",
            "| Steps 4.04e+04  | avgR   147.875  stdR     5.862  | avgS    148\n",
            "| Steps 4.11e+04  | avgR   122.625  stdR     4.794  | avgS    123\n",
            "| Steps 4.89e+04  | avgR   200.000  stdR     0.000  | avgS    200\n",
            "| Steps 4.98e+04  | avgR   194.875  stdR     6.827  | avgS    195\n",
            "| Steps 5.66e+04  | avgR   177.750  stdR    21.376  | avgS    178\n",
            "| Steps 5.72e+04  | avgR   184.750  stdR    14.403  | avgS    185\n",
            "| Steps 6.38e+04  | avgR   200.000  stdR     0.000  | avgS    200\n",
            "| Steps 6.43e+04  | avgR   152.250  stdR     7.446  | avgS    152\n",
            "| Steps 7.04e+04  | avgR   140.000  stdR     3.317  | avgS    140\n",
            "| Steps 7.06e+04  | avgR   120.500  stdR     4.664  | avgS    120\n",
            "| Steps 7.59e+04  | avgR    90.625  stdR     3.199  | avgS     91\n",
            "| Steps 7.60e+04  | avgR   190.625  stdR     9.027  | avgS    191\n",
            "| Steps 8.03e+04  | avgR   190.375  stdR    16.763  | avgS    190\n",
            "| Save learning curve in ./CartPole-v0_DQN_-1/LearningCurve_CartPole-v0_AgentDQN.jpg\n",
            "| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train DQN on [**Discreted action** space env `LunarLander`](https://gym.openai.com/envs/LunarLander-v2/)\n",
        "\n",
        "**You can pass and run codes below.**. Because DQN takes over 1000 seconds for training. It is too slow.\n",
        "\n",
        "And there are many other DQN variance algorithms which get higher cumulative returns and takes less time for training. See [examples/demo_DQN_Dueling_Double_DQN.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/examples/demo_DQN_Dueling_Double_DQN.py)"
      ],
      "metadata": {
        "id": "qK21xTxnHGOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.agent import AgentDQN\n",
        "agent_class = AgentDQN\n",
        "env_name = \"LunarLander-v2\"\n",
        "\n",
        "import gym\n",
        "gym.logger.set_level(40)  # Block warning\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** 0\n",
        "args.gamma = 0.99\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 3\n",
        "\n",
        "args.batch_size = 2 ** 6\n",
        "\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.125\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 7\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(4e5)  # LunarLander needs a larger `break_step`\n",
        "\n",
        "args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of LunarLander-v2 is ∈ (-1800, (-600, 200), 340)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH91VA17Hcsn",
        "outputId": "96b6ee04-4842-44e7-e0b7-6803a24878d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env_args = {'env_num': 1,\n",
            "            'env_name': 'LunarLander-v2',\n",
            "            'max_step': 1000,\n",
            "            'state_dim': 8,\n",
            "            'action_dim': 4,\n",
            "            'if_discrete': True}\n",
            "| Arguments Remove cwd: ./LunarLander-v2_DQN_-1\n",
            "\n",
            "| `Steps` denotes the number of samples, or the total training step, or the running times of `env.step()`.\n",
            "| `ExpR` denotes average rewards during exploration. The agent gets this rewards with noisy action.\n",
            "| `ObjC` denotes the objective of Critic network. Or call it loss function of critic network.\n",
            "| `ObjA` denotes the objective of Actor network. It is the average Q value of the critic network.\n",
            "\n",
            "| Steps 1.02e+03  ExpR    -2.63  | ObjC     1.95  ObjA    -2.92\n",
            "| Steps 1.93e+04  ExpR    -0.52  | ObjC     1.15  ObjA    -4.78\n",
            "| Steps 3.42e+04  ExpR    -0.18  | ObjC     2.60  ObjA   -20.33\n",
            "| Steps 4.70e+04  ExpR    -0.12  | ObjC     2.14  ObjA   -87.57\n",
            "| Steps 5.93e+04  ExpR    -0.04  | ObjC     7.10  ObjA   -14.11\n",
            "| Steps 7.12e+04  ExpR    -0.05  | ObjC     3.57  ObjA     6.03\n",
            "| Steps 8.19e+04  ExpR    -0.10  | ObjC     1.06  ObjA   -53.53\n",
            "| Steps 9.21e+04  ExpR    -0.10  | ObjC    12.48  ObjA    -8.07\n",
            "| Steps 1.04e+05  ExpR    -0.38  | ObjC    13.75  ObjA   -12.73\n",
            "| Steps 1.16e+05  ExpR    -0.44  | ObjC     0.84  ObjA     4.20\n",
            "| Steps 1.27e+05  ExpR    -0.69  | ObjC     0.94  ObjA   -26.14\n",
            "| Steps 1.37e+05  ExpR    -0.40  | ObjC     1.11  ObjA     3.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Train DDPG on continuous action space task.**\n",
        "\n",
        "Train DDPG on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\n",
        "\n",
        "We show a cunstom env in [elegantrl_helloworld/env.py `class PendulumEnv`](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl_helloworld/env.py#L19-L23)\n",
        "\n",
        "OpenAI Pendulum env set its action space as (-2, +2). It is bad. We suggest that adjust action space to (-1, +1) when designing your own env.\n"
      ],
      "metadata": {
        "id": "z2Ik5cDoyPGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentDDPG\n",
        "agent_class = AgentDDPG\n",
        "\n",
        "from elegantrl_helloworld.env import PendulumEnv\n",
        "env = PendulumEnv()\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 2\n",
        "args.net_dim = 2 ** 7\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 0\n",
        "args.explore_noise = 0.1\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(1e5)\n",
        "\n",
        "args.learner_gpus = -1  # denotes use CPU\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwkZXiHtyV6f",
        "outputId": "880d25f5-d1f0-4cd2-8f78-bb5409330101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action_dim': 4,\n",
              " 'env_name': 'BipedalWalker-v3',\n",
              " 'env_num': 1,\n",
              " 'if_discrete': False,\n",
              " 'max_step': 1600,\n",
              " 'state_dim': 24,\n",
              " 'target_return': 300}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 5: Train PPO on continuous action space task.**\n",
        "\n",
        "Train PPO on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E03f6cTeajK4"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "\n",
        "from elegantrl_helloworld.env import PendulumEnv\n",
        "env = PendulumEnv()\n",
        "env_func = PendulumEnv\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.net_dim = 2 ** 7\n",
        "args.num_layer = 2\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 5\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 3\n",
        "args.break_step = int(8e5)\n",
        "\n",
        "args.learner_gpus = gpu_id\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train PPO on [**Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)"
      ],
      "metadata": {
        "id": "rcFcUkwfzHLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "import gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.gamma = 0.99\n",
        "args.reward_scale = 2 ** -1\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step * 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 7\n",
        "args.repeat_times = 2 ** 4\n",
        "args.lambda_entropy = 0.04\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 5\n",
        "args.break_step = int(4e5)\n",
        "\n",
        "args.learner_gpus = gpu_id\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of LunarLanderContinuous-v2 is ∈ (-1800, (-300, 200), 310+)')"
      ],
      "metadata": {
        "id": "9WCAcmIfzGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "Train PPO on [**Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGOPSD6da23k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8ed03b-b306-45f8-c530-adf72438c5bd"
      },
      "source": [
        "from elegantrl_helloworld.config import Arguments\n",
        "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
        "from elegantrl_helloworld.env import get_gym_env_args\n",
        "from elegantrl_helloworld.agent import AgentPPO\n",
        "agent_class = AgentPPO\n",
        "env_name = \"BipedalWalker-v3\"\n",
        "\n",
        "import gym\n",
        "env = gym.make(env_name)\n",
        "env_func = gym.make\n",
        "env_args = get_gym_env_args(env, if_print=True)\n",
        "\n",
        "args = Arguments(agent_class, env_func, env_args)\n",
        "\n",
        "'''reward shaping'''\n",
        "args.reward_scale = 2 ** -1\n",
        "args.gamma = 0.98\n",
        "\n",
        "'''network update'''\n",
        "args.target_step = args.max_step\n",
        "args.net_dim = 2 ** 8\n",
        "args.num_layer = 3\n",
        "args.batch_size = 2 ** 8\n",
        "args.repeat_times = 2 ** 4\n",
        "\n",
        "'''evaluate'''\n",
        "args.eval_gap = 2 ** 6\n",
        "args.eval_times = 2 ** 4\n",
        "args.break_step = int(1e6)\n",
        "\n",
        "args.learner_gpus = gpu_id\n",
        "args.random_seed += gpu_id\n",
        "train_agent(args)\n",
        "evaluate_agent(args)\n",
        "print('| The cumulative returns of BipedalWalker-v3 is ∈ (-150, (-100, 280), 320+)')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Arguments Remove cwd: ./BipedalWalker-v3_PPO_0\n",
            "################################################################################\n",
            "ID     Step    maxR |    avgR   stdR   avgS  stdS |    expR   objC   etc.\n",
            "0  6.98e+03  -91.89 |\n",
            "0  6.98e+03  -91.89 |  -91.89    0.0    109     2 |   -0.39 676.16   0.06  -0.50\n",
            "0  9.49e+04  -21.05 |\n",
            "0  9.49e+04  -21.05 |  -21.05    0.4   1600     0 |   -0.05   6.96   0.02  -0.50\n",
            "0  1.59e+05  -21.05 |  -38.62    1.8   1600     0 |   -0.03   0.34  -0.01  -0.51\n",
            "0  2.24e+05  -21.05 |  -34.80    3.4   1600     0 |   -0.02   0.31   0.05  -0.52\n",
            "0  2.94e+05  133.03 |\n",
            "0  2.94e+05  133.03 |  133.03    4.3   1600     0 |    0.01   0.59  -0.05  -0.53\n",
            "0  3.65e+05  133.03 |  -95.17    0.2    121     7 |    0.04   0.75   0.05  -0.55\n",
            "0  4.55e+05  133.03 | -125.18   13.9    268    68 |    0.07   5.88   0.03  -0.56\n",
            "0  5.37e+05  133.03 |  -63.86   34.8    416   175 |    0.08   7.43  -0.01  -0.57\n",
            "0  6.20e+05  152.64 |\n",
            "0  6.20e+05  152.64 |  152.64  137.1   1152   451 |    0.14   1.71  -0.00  -0.58\n",
            "0  6.94e+05  278.71 |\n",
            "0  6.94e+05  278.71 |  278.71    7.4   1408    58 |    0.14   4.01   0.01  -0.59\n",
            "0  7.67e+05  278.71 |  245.39   81.0   1240   183 |    0.15   5.87   0.04  -0.60\n",
            "0  8.39e+05  278.71 |  162.94  146.0   1047   410 |    0.16   9.38   0.03  -0.60\n",
            "0  9.17e+05  278.71 |  276.64   29.7   1294    24 |    0.16  11.96   0.01  -0.61\n",
            "0  9.93e+05  287.92 |\n",
            "0  9.93e+05  287.92 |  287.92    1.1   1231    23 |    0.12  16.81   0.05  -0.61\n",
            "0  1.07e+06  287.92 |  279.84   29.0   1228    21 |    0.18   8.00   0.00  -0.61\n",
            "0  1.14e+06  289.48 |\n",
            "0  1.14e+06  289.48 |  289.48    1.2   1186    19 |    0.21   2.86   0.05  -0.62\n",
            "0  1.22e+06  289.48 |  248.17   88.0   1105   190 |    0.21   2.38   0.07  -0.63\n",
            "0  1.29e+06  293.11 |\n",
            "0  1.29e+06  293.11 |  293.11    1.4   1118    17 |    0.16  19.98   0.03  -0.63\n",
            "0  1.37e+06  293.11 |  278.33   58.0   1082   104 |    0.23   2.73   0.05  -0.64\n",
            "0  1.44e+06  293.11 |  269.85   64.9   1030   105 |    0.24   4.19  -0.03  -0.64\n",
            "0  1.52e+06  293.11 |  273.76   82.2    988   184 |    0.20   7.97   0.06  -0.64\n",
            "0  1.60e+06  293.11 |  289.13   43.2    983    53 |    0.21  32.67  -0.03  -0.65\n",
            "0  1.67e+06  296.86 |\n",
            "0  1.67e+06  296.86 |  296.86    0.9   1017    12 |    0.25   3.27   0.03  -0.65\n",
            "0  1.75e+06  296.86 |  232.54  103.0    865   176 |    0.21  16.98   0.05  -0.65\n",
            "0  1.83e+06  296.86 |  249.79  107.9    887   207 |    0.25  11.60   0.02  -0.66\n",
            "0  1.91e+06  296.86 |  283.17   51.2    969    76 |    0.23  19.41   0.01  -0.66\n",
            "0  1.98e+06  296.86 |  284.09   46.3    965    62 |    0.25   6.25  -0.02  -0.67\n",
            "0  2.06e+06  296.86 |  281.10   63.2    930   103 |    0.27   6.64   0.04  -0.67\n",
            "0  2.13e+06  296.86 |  244.63  101.8    876   166 |    0.23  22.69   0.07  -0.68\n",
            "0  2.22e+06  296.86 |  229.58  107.9    840   190 |    0.20  26.26   0.01  -0.68\n",
            "0  2.30e+06  296.86 |  265.12   85.9    907   153 |    0.26   8.28   0.05  -0.68\n",
            "0  2.37e+06  300.30 |\n",
            "0  2.37e+06  300.30 |  300.30    1.3    944    19 |    0.27   9.27   0.07  -0.68\n",
            "| UsedTime: 4151 | SavedDir: ./BipedalWalker-v3_PPO_0\n"
          ]
        }
      ]
    }
  ]
}