{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# **Pendulum-v1 Example in ElegantRL-HelloWorld**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 1: Install ElegantRL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 2: Specify Environment and Agent**\n",
        "\n",
        "*   **agent**: chooses a agent (DRL algorithm) from a set of agents in the [directory](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl/agents).\n",
        "*   **env**: creates an environment for your agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1eyRQkk9pkBf"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E03f6cTeajK4"
      },
      "outputs": [],
      "source": [
        "from elegantrl.train.config import Config\n",
        "from elegantrl.agents.AgentSAC import AgentSAC\n",
        "\n",
        "env = gym.make\n",
        "\n",
        "env_args = {\n",
        "    \"id\": \"Pendulum-v1\",\n",
        "    \"env_name\": \"Pendulum-v1\",\n",
        "    \"num_envs\": 1,\n",
        "    \"max_step\": 1000,\n",
        "    \"state_dim\": 3,\n",
        "    \"action_dim\": 1,\n",
        "    \"if_discrete\": False,\n",
        "    \"reward_scale\": 2**-1,\n",
        "    \"gpu_id\": 0, # if you have GPU\n",
        "}\n",
        "args = Config(AgentSAC, env_class=env, env_args=env_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFcUkwfzHLE"
      },
      "source": [
        "# **Part 3: Specify hyper-parameters**\n",
        "A list of hyper-parameters is available [here](https://elegantrl.readthedocs.io/en/latest/api/config.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9WCAcmIfzGyE"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "args.max_step = 1000\n",
        "args.reward_scale = 2**-1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.99\n",
        "args.target_step = args.max_step\n",
        "args.eval_times = 2**3\n",
        "args.num_workers=16 #rollout, improve gpu utilization\n",
        "args.num_threads=2\n",
        "# args.learner_gpu_ids=[0,1]\n",
        "args.net_dims = [64, 64]  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
        "args.learning_rate = 3e-3  # the learning rate for network updating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "# **Part 4: Train and Evaluate the Agent**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOPSD6da23k",
        "outputId": "fd200e35-217c-41d6-b01c-e4a8a535840c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| train_agent_multiprocessing() with GPU_ID 0\n",
            "| Arguments Remove cwd: runs/Pendulum-v1_SAC_2\n",
            "| Evaluator:\n",
            "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
            "| `time`: Time spent from the start of training to this moment.\n",
            "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `avgS`: Average of steps in an episode.\n",
            "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
            "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
            "################################################################################\n",
            "ID     Step    Time |    avgR   stdR   avgS  stdS |    expR   objC   objA   etc.\n",
            "0  1.64e+04     100 |-1033.51  362.5    200     0 |   -3.05   2.98  -0.10 -0.09649907250422984\n",
            "0  5.73e+04     110 |-1356.23  329.8    200     0 |   -3.29   0.12  -2.30 -2.296949341893196\n",
            "0  8.19e+04     132 |-1532.33  192.8    200     0 |   -3.28   0.05  -5.04 -5.040051817893982\n",
            "0  1.06e+05     155 |-1437.38   81.1    200     0 |   -3.66   0.05  -9.15 -9.153872288190401\n",
            "0  1.31e+05     179 |-1423.54  165.2    200     0 |   -3.70   0.07 -14.43 -14.428920321166515\n",
            "0  1.56e+05     205 |-1309.24  162.5    200     0 |   -3.35   0.10 -21.20 -21.200280716544704\n",
            "0  1.80e+05     231 |-1210.73  148.3    200     0 |   -2.99   0.21 -28.50 -28.49563352628188\n",
            "0  2.05e+05     259 |-1028.86  270.9    200     0 |   -2.88   0.36 -36.57 -36.56949094772339\n",
            "0  2.29e+05     288 | -955.53   93.3    200     0 |   -2.45   0.68 -44.86 -44.86368574414934\n",
            "0  2.54e+05     317 |-1137.54  127.3    200     0 |   -2.57   1.14 -53.89 -53.89216000033963\n",
            "0  2.79e+05     348 |-1090.28   76.3    200     0 |   -2.60   1.38 -63.84 -63.84367865674636\n",
            "0  3.03e+05     379 | -656.79  338.6    200     0 |   -2.45   1.71 -74.59 -74.58939400234738\n",
            "0  3.28e+05     412 | -413.34  233.1    200     0 |   -1.94   1.84 -85.61 -85.60513331890107\n",
            "0  3.52e+05     446 | -366.24  332.7    200     0 |   -1.93   2.15 -96.08 -96.08011425373167\n",
            "0  3.77e+05     481 | -559.97  301.5    200     0 |   -1.58   2.96-106.78 -106.78335934099944\n",
            "0  4.01e+05     517 | -760.11  445.0    200     0 |   -1.80   3.99-118.15 -118.1542834457086\n",
            "0  4.26e+05     553 | -747.64  607.2    200     0 |   -1.39   5.18-128.67 -128.67109608650208\n",
            "0  4.51e+05     591 | -606.26  562.9    200     0 |   -1.57   6.55-138.62 -138.6220962524414\n",
            "0  4.75e+05     630 | -548.89  454.4    200     0 |   -1.95   8.16-149.19 -149.18971979207006\n",
            "0  5.00e+05     669 | -377.57  433.8    200     0 |   -1.67  10.15-160.34 -160.33540575621558\n",
            "0  5.24e+05     710 | -455.20  503.2    200     0 |   -1.81  12.04-171.17 -171.1743679344654\n",
            "0  5.49e+05     752 | -543.25  532.7    200     0 |   -1.95  15.03-181.25 -181.25482308686668\n",
            "0  5.73e+05     795 | -306.03  433.5    200     0 |   -2.11  17.88-192.64 -192.6426339013236\n",
            "0  5.98e+05     839 | -619.29  463.6    200     0 |   -1.87  19.75-202.96 -202.96456666188698\n",
            "0  6.23e+05     884 | -702.37  621.6    200     0 |   -1.49  22.33-212.65 -212.65206818831595\n",
            "0  6.47e+05     930 | -561.16  470.1    200     0 |   -1.70  24.92-222.76 -222.75934064840968\n",
            "0  6.72e+05     976 | -649.26  510.9    200     0 |   -1.74  27.02-231.88 -231.87978609596811\n",
            "0  6.96e+05    1027 | -593.40  646.6    200     0 |   -2.03  30.06-241.54 -241.53617098191205\n",
            "0  7.21e+05    1075 | -408.52  456.1    200     0 |   -1.72  32.54-249.08 -249.07707333564758\n",
            "0  7.45e+05    1126 | -754.30  576.8    200     0 |   -1.49  35.14-258.05 -258.05231639317105\n",
            "0  7.70e+05    1177 | -458.74  405.4    200     0 |   -1.90  37.12-265.61 -265.6071219748639\n",
            "0  7.95e+05    1229 | -242.60  297.9    200     0 |   -1.87  39.47-273.64 -273.64252515183284\n",
            "0  8.19e+05    1283 | -430.35  514.6    200     0 |   -1.84  41.98-280.11 -280.10590364456175\n",
            "0  8.44e+05    1337 | -545.60  452.0    200     0 |   -1.80  44.72-287.69 -287.6936939943184\n",
            "0  8.68e+05    1393 | -513.51  449.8    200     0 |   -1.85  45.97-292.61 -292.61464676767025\n",
            "0  8.93e+05    1450 | -397.11  436.5    200     0 |   -1.63  48.64-298.28 -298.2774311555635\n",
            "0  9.18e+05    1507 | -465.48  552.2    200     0 |   -1.50  51.03-304.91 -304.90815676961626\n",
            "0  9.42e+05    1566 | -532.87  595.4    200     0 |   -1.99  51.35-307.29 -307.2876030134118\n",
            "0  9.67e+05    1627 | -732.55  494.2    200     0 |   -1.28  53.62-310.34 -310.3396214307365\n",
            "0  9.91e+05    1691 | -555.28  499.9    200     0 |   -1.77  54.82-312.42 -312.4211637481185\n",
            "0  1.02e+06    1756 | -734.49  523.5    200     0 |   -1.91  57.56-314.95 -314.9487662469187\n"
          ]
        }
      ],
      "source": [
        "from elegantrl.train.run import train_agent\n",
        "\n",
        "train_agent(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPXOxLSqh5cP"
      },
      "source": [
        "Understanding the above results::\n",
        "*   **Step**: the total training steps.\n",
        "*  **MaxR**: the maximum reward.\n",
        "*   **avgR**: the average of the rewards.\n",
        "*   **stdR**: the standard deviation of the rewards.\n",
        "*   **objA**: the objective function value of Actor Network (Policy Network).\n",
        "*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "quickstart_Pendulum-v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
