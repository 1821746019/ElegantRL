{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gUG3OCJ5GS"
      },
      "source": [
        "# **Pendulum-v1 Example in ElegantRL-HelloWorld**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbamGVHC3AeW"
      },
      "source": [
        "# **Part 1: Install ElegantRL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8zcgcn14uq"
      },
      "source": [
        "# **Part 2: Specify Environment and Agent**\n",
        "\n",
        "*   **agent**: chooses a agent (DRL algorithm) from a set of agents in the [directory](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl/agents).\n",
        "*   **env**: creates an environment for your agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1eyRQkk9pkBf"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E03f6cTeajK4"
      },
      "outputs": [],
      "source": [
        "from elegantrl.train.config import Config\n",
        "from elegantrl.agents.AgentPPO import AgentDiscretePPO\n",
        "\n",
        "env = gym.make\n",
        "\n",
        "env_args = {\n",
        "    \"id\": \"CartPole-v1\",\n",
        "    \"env_name\": \"CartPole-v1\",\n",
        "    \"num_envs\": 1,\n",
        "    \"max_step\": 1000,\n",
        "    \"state_dim\": 4,\n",
        "    \"action_dim\": 2,\n",
        "    \"if_discrete\": True,\n",
        "    \"reward_scale\": 2**-1,\n",
        "}\n",
        "args = Config(AgentDiscretePPO, env_class=env, env_args=env_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFcUkwfzHLE"
      },
      "source": [
        "# **Part 3: Specify hyper-parameters**\n",
        "A list of hyper-parameters is available [here](https://elegantrl.readthedocs.io/en/latest/api/config.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9WCAcmIfzGyE"
      },
      "outputs": [],
      "source": [
        "args.max_step = 1000\n",
        "args.reward_scale = 2**-1  # RewardRange: -1800 < -200 < -50 < 0\n",
        "args.gamma = 0.97\n",
        "args.target_step = args.max_step\n",
        "args.eval_times = 2**3\n",
        "args.num_workers=16 #rollout, improve gpu utilization\n",
        "args.num_threads=2\n",
        "args.gpu_id=1\n",
        "args.break_step=1e6 #1m步\n",
        "args.net_dims = [64, 64]  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
        "args.learning_rate = 3e-4  # the learning rate for network updating\n",
        "\n",
        "# args.learner_gpu_ids=[0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1j5kLHF2dhJ"
      },
      "source": [
        "# **Part 4: Train and Evaluate the Agent**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOPSD6da23k",
        "outputId": "fd200e35-217c-41d6-b01c-e4a8a535840c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| train_agent_multiprocessing() with GPU_ID 1\n",
            "| Arguments Remove cwd: runs/CartPole-v1_DiscretePPO_0\n",
            "| Evaluator:\n",
            "| `step`: Number of samples, or total training steps, or running times of `env.step()`.\n",
            "| `time`: Time spent from the start of training to this moment.\n",
            "| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\n",
            "| `avgS`: Average of steps in an episode.\n",
            "| `objC`: Objective of Critic network. Or call it loss function of critic network.\n",
            "| `objA`: Objective of Actor network. It is the average Q value of the critic network.\n",
            "################################################################################\n",
            "ID     Step    Time |    avgR   stdR   avgS  stdS |    expR   objC   objA   etc.\n",
            "1  6.55e+04     162 |  396.00  133.5    396   134 |   -0.68   8.50  -0.18   0.59 0.5915502104908228\n",
            "1  1.31e+05     208 |  404.12  107.1    404   107 |   -0.53   3.57  -0.11   0.48 0.4832564184907824\n",
            "1  1.64e+05     239 |  291.12  166.6    291   167 |   -0.50   1.30  -0.12   0.49 0.48824046016670763\n",
            "1  1.97e+05     282 |  440.12   70.9    440    71 |   -0.50   0.52  -0.15   0.48 0.48255920107476413\n",
            "1  2.29e+05     305 |  230.00   31.0    230    31 |   -0.50   0.61  -0.11   0.51 0.5090925204567611\n",
            "1  2.62e+05     348 |  267.25   24.5    267    25 |   -0.51   0.39  -0.10   0.47 0.4705588296055794\n",
            "1  2.95e+05     390 |  487.62   21.2    488    21 |   -0.49   0.31  -0.18   0.48 0.48192193545401096\n",
            "1  3.28e+05     420 |  326.25   78.9    326    79 |   -0.49   0.27  -0.20   0.48 0.48494851775467396\n",
            "1  3.60e+05     456 |  309.62   14.2    310    14 |   -0.48   0.06  -0.10   0.37 0.37293824600055814\n",
            "1  3.93e+05     494 |  357.75   33.9    358    34 |   -0.51   1.38  -0.15   0.50 0.49827483645640314\n",
            "1  4.26e+05     529 |  500.00    0.0    500     0 |   -0.50   1.03  -0.14   0.46 0.46132869250141084\n",
            "1  4.59e+05     561 |  248.00   14.4    248    14 |   -0.46   0.09  -0.13   0.39 0.38755758362822235\n",
            "1  4.92e+05     607 |  500.00    0.0    500     0 |   -0.44   0.92  -0.20   0.41 0.4081759911496192\n",
            "1  5.24e+05     643 |  500.00    0.0    500     0 |   -0.44   0.61  -0.12   0.39 0.3850455565843731\n",
            "1  5.57e+05     681 |  500.00    0.0    500     0 |   -0.43   0.74  -0.07   0.40 0.40430515818297863\n",
            "1  5.90e+05     715 |  500.00    0.0    500     0 |   -0.40   0.06  -0.17   0.38 0.3843045763205737\n",
            "1  6.23e+05     751 |  500.00    0.0    500     0 |   -0.42   0.43  -0.09   0.40 0.4031874251086265\n",
            "1  6.55e+05     788 |  500.00    0.0    500     0 |   -0.41   0.04  -0.18   0.40 0.3958796539809555\n",
            "1  6.88e+05     824 |  500.00    0.0    500     0 |   -0.39   0.01  -0.16   0.35 0.3456614303868264\n",
            "1  7.21e+05     861 |  500.00    0.0    500     0 |   -0.38   0.50  -0.11   0.37 0.37472020648419857\n",
            "1  7.54e+05     896 |  500.00    0.0    500     0 |   -0.38   0.21  -0.09   0.38 0.3754585557617247\n",
            "1  7.86e+05     932 |  500.00    0.0    500     0 |   -0.38   0.03  -0.19   0.37 0.3653986076824367\n",
            "1  8.19e+05     967 |  500.00    0.0    500     0 |   -0.39   0.01  -0.18   0.39 0.39459265512414277\n",
            "1  8.52e+05    1003 |  500.00    0.0    500     0 |   -0.39   0.00  -0.14   0.39 0.3942388170398772\n",
            "1  8.85e+05    1039 |  500.00    0.0    500     0 |   -0.39   0.00  -0.15   0.39 0.3867727315519005\n",
            "1  9.18e+05    1073 |  500.00    0.0    500     0 |   -0.39   0.00  -0.14   0.38 0.38102180953137577\n",
            "1  9.50e+05    1111 |  500.00    0.0    500     0 |   -0.38   0.00  -0.14   0.38 0.37965779029764235\n",
            "1  9.83e+05    1147 |  500.00    0.0    500     0 |   -0.38   0.00  -0.12   0.38 0.3758751223795116\n",
            "1  1.02e+06    1183 |  500.00    0.0    500     0 |   -0.37   0.00  -0.12   0.33 0.3272077632136643\n",
            "1  1.05e+06    1219 |  500.00    0.0    500     0 |   -0.40   0.07  -0.18   0.34 0.33975906448904425\n",
            "1  1.08e+06    1254 |  500.00    0.0    500     0 |   -0.39   0.00  -0.16   0.37 0.366270161815919\n",
            "1  1.11e+06    1290 |  500.00    0.0    500     0 |   -0.43   0.06  -0.17   0.42 0.41697897645644844\n",
            "1  1.15e+06    1327 |  500.00    0.0    500     0 |   -0.41   0.00  -0.13   0.40 0.398043958703056\n",
            "1  1.18e+06    1361 |  500.00    0.0    500     0 |   -0.40   0.00  -0.13   0.38 0.3839777675457299\n"
          ]
        }
      ],
      "source": [
        "from elegantrl.train.run import train_agent\n",
        "\n",
        "train_agent(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPXOxLSqh5cP"
      },
      "source": [
        "Understanding the above results::\n",
        "*   **Step**: the total training steps.\n",
        "*  **MaxR**: the maximum reward.\n",
        "*   **avgR**: the average of the rewards.\n",
        "*   **stdR**: the standard deviation of the rewards.\n",
        "*   **objA**: the objective function value of Actor Network (Policy Network).\n",
        "*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "quickstart_Pendulum-v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
